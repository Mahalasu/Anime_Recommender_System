{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'all_rating_min_max',\n",
    "        'members_min_max',\n",
    "        'aired_from_min_max',\n",
    "        'aired_to_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()\n",
    "\n",
    "def get_user_numeric_features(df):\n",
    "    numeric_feature_names = [\n",
    "        'user_rating_ave_min_max',\n",
    "        'user_rating_std_min_max',\n",
    "        'user_aired_from_ave_min_max',\n",
    "        'user_aired_to_ave_min_max'\n",
    "    ]\n",
    "    \n",
    "    num_df = df[numeric_feature_names]\n",
    "    return num_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multihot_feature(df, feat_name):\n",
    "    feat_df = df[[feat_name]]\n",
    "    feat_vecs = feat_df.to_numpy()\n",
    "    feat_vec = np.apply_along_axis(lambda v: v[0], 1, feat_vecs)\n",
    "    return feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(df):\n",
    "    label_df = df[['label']]\n",
    "    return label_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_features(df):\n",
    "    return ( \n",
    "        get_multihot_feature(df, 'genres_multihot'),\n",
    "        get_multihot_feature(df, 'user_liked_genres_multihot'),\n",
    "        get_item_numeric_features(df),\n",
    "        get_user_numeric_features(df) \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Parquet Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('../data/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP_LAYERS = hp.HParam(\"layers\", hp.IntInterval(2, 3))\n",
    "HP_LAYER_SIZE = hp.HParam(\"layer_size\", hp.Discrete([64, 128, 256]))\n",
    "HP_LEARN_RATE = hp.HParam(\"learn_rate\", hp.Discrete([0.001, 0.003, 0.01]))\n",
    "\n",
    "HPARAMS = [\n",
    "    HP_LAYERS,\n",
    "    HP_LAYER_SIZE,\n",
    "    HP_LEARN_RATE\n",
    "]\n",
    "\n",
    "METRICS = [\n",
    "    hp.Metric(\n",
    "        \"batch_loss\",\n",
    "        group=\"train\",\n",
    "        display_name=\"loss (train)\",\n",
    "    ),\n",
    "    hp.Metric(\n",
    "        \"loss\",\n",
    "        group=\"validation\",\n",
    "        display_name=\"loss (val)\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x1_shape, x2_shape, x3_shape, x4_shape, hparams):\n",
    "    x1_input = keras.layers.Input(shape=(x1_shape,))\n",
    "    x2_input = keras.layers.Input(shape=(x2_shape,))\n",
    "    x3_input = keras.layers.Input(shape=(x3_shape,))\n",
    "    x4_input = keras.layers.Input(shape=(x4_shape,))\n",
    "    \n",
    "    # compact embedding for x1 and x2\n",
    "    compact_x1 = keras.layers.Dense(10)(x1_input)\n",
    "    compact_x2 = keras.layers.Dense(10)(x2_input)\n",
    "    \n",
    "    # concat all\n",
    "    merge = keras.layers.concatenate([compact_x1, compact_x2, x3_input, x4_input])\n",
    "    \n",
    "    # hidden layers\n",
    "    h_input = merge\n",
    "    for _ in range(hparams[HP_LAYERS]):\n",
    "        h = keras.layers.Dense(hparams[HP_LAYER_SIZE], activation='relu')(h_input)\n",
    "        h_input = h\n",
    "    \n",
    "    # output\n",
    "    output = keras.layers.Dense(1, activation='sigmoid')(h_input)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[x1_input, x2_input, x3_input, x4_input], outputs=output)\n",
    "    \n",
    "    # optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=hparams[HP_LEARN_RATE])\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=opt,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x1s = []\n",
    "test_x2s = []\n",
    "test_x3s = []\n",
    "test_x4s = []\n",
    "test_ys = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_files():\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('../data/dnn_feat_eng'):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                filenames.append(os.path.join(root, file))\n",
    "                \n",
    "    return filenames\n",
    "\n",
    "filenames = data_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model_id, hparams):\n",
    "    # build model\n",
    "    model = build_model(43, 43, 4, 4, hparams)\n",
    "    print(f\"model id: {model_id}:\")\n",
    "    print({h.name: hparams[h] for h in hparams})\n",
    "\n",
    "    # config hparam logs\n",
    "    log_filename = f\"{model_id}\"\n",
    "    for h in hparams:\n",
    "        log_filename += f\"_{h.name}-{hparams[h]}\"\n",
    "    \n",
    "    log_dir = os.path.join(\"hparams\", log_filename)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir = log_dir,\n",
    "        update_freq = 10,\n",
    "        profile_batch = 0\n",
    "    )\n",
    "    hparams_callback = hp.KerasCallback(log_dir, hparams)\n",
    "    \n",
    "    # train model\n",
    "    for filename in filenames[:1]:\n",
    "        df = pd.read_parquet(filename)\n",
    "\n",
    "        # shuffle and split train and test\n",
    "        train_df = df\n",
    "\n",
    "        # get features\n",
    "        train_x1, train_x2, train_x3, train_x4 = get_all_features(train_df)\n",
    "\n",
    "        # get label\n",
    "        train_y = get_label(train_df)\n",
    "\n",
    "        print('training on new dataset')\n",
    "\n",
    "        model.fit(\n",
    "            [train_x1, train_x2, train_x3, train_x4], \n",
    "            train_y, \n",
    "            validation_split=0.2,\n",
    "            batch_size=16, \n",
    "            epochs=4,\n",
    "            callbacks=[tensorboard_callback, hparams_callback]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_params():\n",
    "    with tf.summary.create_file_writer('hparams').as_default():\n",
    "            hp.hparams_config(hparams=HPARAMS, metrics=METRICS)\n",
    "            \n",
    "    model_id = 0\n",
    "    for layers in range(HP_LAYERS.domain.min_value, HP_LAYERS.domain.max_value + 1):\n",
    "        for size in HP_LAYER_SIZE.domain.values:\n",
    "            for rate in HP_LEARN_RATE.domain.values:\n",
    "                hparams = {\n",
    "                    HP_LAYERS: layers,\n",
    "                    HP_LAYER_SIZE: size,\n",
    "                    HP_LEARN_RATE: rate\n",
    "                }\n",
    "\n",
    "                run_model(model_id, hparams)\n",
    "                model_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%rm -rf hparams\n",
    "test_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x1 = np.vstack(test_x1s)\n",
    "test_x2 = np.vstack(test_x2s)\n",
    "test_x3 = np.vstack(test_x3s)\n",
    "test_x4 = np.vstack(test_x4s)\n",
    "test_y = np.vstack(test_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate([test_x1, test_x2, test_x3, test_x4], test_y)\n",
    "\n",
    "print('\\n\\nTest Loss {}, Test Accuracy {}'.format(test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    HP_LAYERS: 2,\n",
    "    HP_LAYER_SIZE: 128,\n",
    "    HP_LEARN_RATE: 0.003,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(43, 43, 4, 4, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: mlp_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('mlp_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f18d6d85afa27dc79607f207c49cc7922ddaa66e675d81ef99263abf5db2e8cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
